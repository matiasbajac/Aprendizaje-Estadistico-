---
title: "Laboratorio 2"
author: "Matias Bajac"
date: '2024-09-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Librerias 
```{r}
library(tidymodels)
library(ISLR2)
```



Haremos el ejercicio 9 (modificado) del capitulo 6 del libro ISL. Predicir el numero de Apps recibidas usando las otras variables del conjunto de datos **Collage** como predictores. 

1) Cargar el conjunto de datos **Collage**, del paquete **ISLR2

```{r}

library(ISLR2)
set.seed(1234)

college_split <- initial_split(College, 
                             
                              prop = 0.80) 
college_train <- training(college_split)

college_test <- testing(college_split)
```


2) Ajustar un modelo lineal usando minimos cuadrados usando **parsnip**.
Reportar el error obtenido en el conjunto de testeo y entrenamiento usando **augment**. Alternativamente especifica y ajusta el modelo usando **workflow**

Primera forma:

```{r}
lm_spec = parsnip::linear_reg() %>%  set_engine("lm")


## Ajusto el modelo 
college_lm_fit = lm_spec %>%  parsnip::fit(Apps ~., data = college_train)

bind_rows(training = augment(college_lm_fit, new_data = college_train) %>%  rmse(Apps,.pred), 
 testing = augment(college_lm_fit, new_data= college_test) %>%  rmse(Apps,.pred))                                                                         
  
  
                                                                                  
```
Segunda forma: 

Especificamos usando workflow() 

```{r}
lm_mod = linear_reg() %>%  set_engine("lm")

a =   workflow() |>
  add_formula(Apps ~ .,) |>
  add_model(lm_mod)

  
  
fit = a %>%  fit(data = college_train) %>%  extract_fit_engine()

tidy(train)

 bind_rows(
   training = augment(fit,new_data = college_train ) %>%  rmse(Apps, .pred), 
   testing   = augment( fit, college_test) %>%  rmse(Apps,.pred))
  
```

3) Ajustar un modelo lineal usando Ridge. Especificar el modelo usando **parsnip**, con **glmnet** como engine y el parametro de penalidad igual a 0. 
Mostrar el ajuste del modelo y reportar el error obtenido en el conjunto de teteo y entrenamiento usando **augment**

```{r}


ridge_spec  <-
 linear_reg(penalty = 0, mixture=0)|>     set_mode("regression") %>%  

 set_engine("glmnet") 

ridge_fit = ridge_spec %>%  fit(Apps~., data = college_train)

tidy(ridge_fit)

bind_rows(
   training = augment(ridge_fit,new_data = college_train ) %>%  rmse(Apps, .pred), 
   testing   = augment( ridge_fit, college_test) %>%  rmse(Apps,.pred))
```
4) Usando **autoplot** defini entre que valores posibles puede estar $\lambda$ y reestima el modelo con ese valor. 

Defninimos el parametro de penalidad, privaty yes es importrante.
vemos la grilla 

encontrar el lambda optimo


```{r}
ridge_fit %>%  autoplot()
tidy(ridge_fit,penalty = 
     1000)
```

5) Procesar los datos con **recipe** incluir dummies para todos los predictores nominales usando **step_dummy** y all_nominal_predictors amdeas de estandarizar todos los predictores. Guardar los datos procesados en un objeto **collegue_recipe**


```{r}
college_recipe = recipes::recipe(Apps~., data= college_train) %>%  recipes::step_dummy(all_nominal_predictors()) %>%  recipes::step_normalize(all_predictors())
```
6) Ajustar un modelo lineal usando Ridge, especifica rel modelo usando parsnip, con glmnet con los datos procesados. Escoger $\lambda$ con CV. Reportar el error obtenido en el conjunto de testeo. Usa la info de autoplot para definir la grilla de lambda


t() el parametro de reg lineal 

```{r}
ridge_spec <- parsnip::linear_reg(penalty = tune(), mixture= 0) %>%  parsnip::set_engine("glmnet")

ridge_wf = workflows::workflow() %>%  workflows:: add_recipe(college_recipe) %>%  workflows::add_model(ridge_spec)

ridge_grid<- dials::grid_regular(penalty(range=c(--5,3)),  levels = 50)

## particiono la muestra en 5 folds para CV 

college_cv = rsample::vfold_cv(college_train,v=5)

ridge_tune  = tune::tune_grid( 
ridge_wf,
  resamples = college_cv, ## particiones 
  grid = ridge_grid ## grilla
)


## particiono la muestra en k = 5, para CV

```

```{r}
ridge_tune |>
  collect_metrics() |>
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")
```



```{r}
autoplot(ridge_tune)
```



```{r}
final_ridge = tune::finalize_workflow(
  ridge_wf,
  tune::select_best(ridge_tune,metric = "rmse")
)

ridge_last_fit = tune::last_fit(object = final_ridge,
                                split = college_split)

ridge_last_fit %>%  collect_metrics()
```


```{r}
# Si no especifico nada usa por defecto range = c(-10,0) en escala log10

ridge_grid <- dials::grid_regular(penalty(), levels = 50)

# En la escala  original en base a exploracion con autoplot
ridge_grid <- dials::grid_regular(penalty(range = c(0.1, 1000), trans = NULL), levels = 50)


ridge_grid <- dials::grid_random(penalty(c(0.1, 1000)), size = 100)

# en log10
ridge_grid <- dials::grid_regular(penalty(range = c(-5, 3)), levels = 50)
```



```{r}
# Actualiza  el workflow con  con el mejor parametro seleccionado

final_ridge <- tune::finalize_workflow(
  ridge_wf,
  tune::select_best(ridge_tune, metric = "rmse")
)

# Ajusta el modelo con el workflow actualizado con el training completo y evalua con el tes, ledamos college split
ridge_last_fit <- tune::last_fit(
  object= final_ridge,
  split=college_split
)

ridge_last_fit |> collect_metrics()
```



7) Ajustar un modelo lineal usando Lasso. Escoger $\lambda$ con validacion cruzada.
Reportar el rmse obtenido en el conjunto de testeo.
```{r}
lasso_spec <- linear_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

lasso_workflow <- workflow() |>
  add_recipe(college_recipe) |>
  add_model(lasso_spec)

lasso_grid <- grid_regular(penalty(range = c(-5, 3)), levels = 50)

lasso_tune <- tune_grid(
  lasso_workflow,
  resamples = college_cv,
  grid = lasso_grid
)

lasso_final <- finalize_workflow(
  lasso_workflow,
  select_best(lasso_tune, metric = "rmse")
)

lasso_last_fit <- last_fit(
  lasso_final, split = college_split
)

lasso_last_fit |> collect_metrics()
```


el parametro puede ser  0
```{r}

lasso_spec <- linear_reg(penalty = tune(), mixture = 1) |> #Con tune() indicamos parÃ¡metro a ser ajustado.
  set_engine("glmnet") 


lasso_workflow = workflow() %>%  add_recipe(college_recipe) %>%  add_model(lasso_spec)

college_cv <- vfold_cv(college_train, v = 5)


lasso_grid <- grid_regular(penalty(c(-5,-3)),  levels = 50)



lasso_grid <- tune_grid( #Realizamos el ajuste, pensar que estamos haciendo 5*50=250 ajustes!
lasso_workflow,
  resamples = college_cv, ## particiones, es el mismo que tenia antes 
  grid = lasso_grid ## grilla
)



best_lambda <- lasso_grid%>%
  select_best(metric = "rmse")

lowest_rmse <- lasso_grid |>
  select_best(metric = "rmse")

final_lasso <- finalize_workflow(tune_wf, lowest_rmse)






```


8) Ajustar un modelo de vecinos mas cercanos. Escoger el k con validacion cruzada.
Reportar el error obtenido en el conjunto de testo.
tunear el k 
```{r}
knn_spec = nearest_neighbor(neighbors = tune()) %>%  set_engine("kknn") %>%  set_mode("regression")

knn_workflow = workflow() %>%  add_recipe(college_recipe) %>%  add_model(knn_spec)

## cambiamos solo el modelo y la grilla 




```



