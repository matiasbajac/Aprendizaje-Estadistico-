---
title: "Laboratorio 2"
author: "Matias Bajac"
date: '2024-09-16'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Validacion cruzada aplicada a lasso
remuestreo

```{r}


library(tidymodels) #Recordar que al importar tidymodels estamos importando varios paquetes
library(schrute) #Contiene los datos que vamos a utilizar, es necesario instalarlo.

install.packages("vip")
library(vip) #Importancia de variables
set.seed(1234)
```

```{r}
office_info <- theoffice |> 
    select(season, episode_name, director, writer,
character, text, imdb_rating)
office_info |> head()
```
## creacion de atributos 
## diferencia entre count y add_count 
## el count te cuenta la cantidad de obs x linea.
## add count suma las lineas por temporadas, es como agrupado por caracter.
## pivot_wider numero de cap para cada personaje 
```{r}
characters <- office_info |>
  count(episode_name, character) |> # Cantidad de lineas por capitulo de cada personaje, contamos las lineas de cap x personas 
  add_count(character, wt = n, name = "character_count") |> #Cantidad de lineas en toda la serie por personaje.
  filter(character_count > 800) |> #Obtenemos los personajes que tengan al menos 800 lineas
  select(-character_count) |>
  pivot_wider(
    names_from = character,
    values_from = n,
    values_fill = list(n = 0)
  ) #Obtenemos una fila sola por capitulo, ponemos 0 si el personaje no tuvo dialogos en el episodio.
```
## menores a 10 son invitados,
## transforma los datos  los re estructura.
## variable de resupesta es raiting 

```{r}
creators <- office_info |>
  distinct(episode_name, director, writer) |>
  pivot_longer(director:writer, names_to = "role", values_to = "person") |>
  separate_rows(person, sep = ";") |>
  add_count(person) |>
  mutate(person = case_when(
    n <= 10 ~ 'Guest',
    n > 10 ~ person
  )) |>
  distinct(episode_name, person) |>
  mutate(person_value = 1) |>
  pivot_wider(
    names_from = person,
    values_from = person_value,
    values_fill = list(person_value = 0)
  )
```

```{r}
office <- office_info |> 
  distinct(season, episode_name, imdb_rating) |>
  inner_join(characters) |>
  inner_join(creators) |>
  mutate_at("season", as.factor)
```

```{r}
office |>
  ggplot(aes(season, imdb_rating, fill = as.factor(season))) +
  geom_boxplot(show.legend = FALSE)
```
El raiting es por cap y agrupar por temporada 
```{r}
office_split <- initial_split(office, 
                              strata = season,
                              prop = 3/4) 
office_train <- training(office_split)
office_test <- testing(office_split)
```
## ver el desbalance de datos



```{r}
office_rec <- recipe(imdb_rating ~ ., data = office_train) |>
  update_role(episode_name, new_role = "ID") |>
  step_dummy(season) |> #codificamos categorias a columnas binarias donde 1 indica que pertenece a esa clase y 0 que no.
  step_normalize(all_numeric(), -all_outcomes()) #Normalizamos los predictores i.e. media = 0, sd = 1.
```

update_role()  sirve para que edntiifqie el cap y no lo conidere predictora ni variable de salida 
step_dummy() recodifica a dummy 

normalizamos porque se tiene distinta unidad de medida 

## parsnip
Define el modelo, y luego con que paquete lo implemento 
penalizar , miniminice el error
no esta ajustando 'un carajo'
Uso como paquete el glmmet

```{r}
tune_spec <- linear_reg(penalty = tune(), mixture = 1) |> #Con tune() indicamos parámetro a ser ajustado.
  set_engine("glmnet") 
```
## workflow 
primero le pongo la receta y despues ajusto

```{r}
tune_wf <- workflow() |>
  add_recipe(office_rec) |>
  add_model(tune_spec)
```

## Ajustamos

grilla busca desde donde a donde buscar 
tenemos una dim sola que es lambda 

 el argumento  trans nulo lo pone en escala original 
```{r}
office_cv <- vfold_cv(office_train, v = 5) #5 particiones debido a que son pocos datos. 
lambda_grid <- grid_regular(penalty(c(-10,-1)),  levels = 50) #Definimos la grilla

lasso_grid <- tune_grid( #Realizamos el ajuste, pensar que estamos haciendo 5*50=250 ajustes!
  tune_wf,
  resamples = office_cv, ## particiones 
  grid = lambda_grid ## grilla
)
```

 separa la muestra en k folds y calcula el mean square error en cada particion 
el fit esta en tune_wf, primero tuneamos el parametro de penalidad 
ajustamos es un conjunto de validacion 
particiones del conjunto de datos da office_cv

```{r}
lasso_grid |>
  collect_metrics() |>
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")


```

```{r}
lasso_grid %>%  collect_metrics()
```
cuanto es la media en el cross validacion en la particion
son 50 hiperparametros x 2 (rsq y rsme)

En el grafico, donde es mas bajo el rsq 
en el codo para mse 

Cual es el mejor en rmse mas chico 
```{r}
lowest_rmse <- lasso_grid |>
  select_best(metric = "rmse")

final_lasso <- finalize_workflow(tune_wf, lowest_rmse)
```
que el error este menos de un sd del mejor. 
modelo simple con mas vecinos 
lasso oficia como seleccion de variables 


## Bootstrap
evalua las que quedan fuera de la muestra 
por defecto la muestra boot es 25
```{r}
office_boot <- bootstraps(office_train, strata = season)
lambda_grid <- grid_regular(penalty(c(-10,-1)), levels = 50) #Definimos la grilla

lasso_grid <- tune_grid(
  tune_wf,
  resamples = office_boot,
  grid = lambda_grid
)
```

## ajusta el modelo final con el mejor hiper en el entrenamiento 
```{r}
last_fit(
  final_lasso,
  office_split
) |>
  collect_metrics()
```
## interpretacion 
importancia de variable 

```{r}
final_lasso |>
  fit(office_train) |>
  extract_fit_engine() |>
  vi(lambda = lowest_rmse$penalty) |> #Es muy importante marcar el lambda!
  ggplot(aes(x = Importance, y = reorder(Variable, Importance), fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```

tiene importancia elegir el beta comoo importancia de variable ya qeue esta estandarizadas las variables 
con lasso los cofiecientes pueden dar 0 , mata a toda las que no estan pintadas 
importance es los beta 
## actividad 
1) Cargar los datos Collage, del paquete ISLR2. 
Dividimos los datos en train - test 
```{r}


college_split <- initial_split(College, 
                             
                              prop = 3/4) 
college_train <- training(college_split)

college_test <- testing(college_split)

```


2) Ajustar un **modelo lineal** usando MCO,reportar el error obteniendo 
receta.



```{r}
##  recipe() sirve para el procesamiento de datos 

college_rec <- recipe(Apps ~ ., data = college_train) |>

  step_dummy(Private) |> #codificamos categorias a columnas binarias donde 1 indica que pertenece a esa clase y 0 que no.
  step_normalize(all_numeric(), -all_outcomes()) #Normalizamos los predictores i.e. media = 0, sd = 1.

tune_spec <- linear_reg(penalty = tune(), mixture = 0) |> #Con tune() indicamos parámetro a ser ajustado.
  ## el unico parametrio que tenemos es penalty
  set_engine("glmnet") 

tune_wf <- workflow() |>
  add_recipe(college_rec) |>
  add_model(tune_spec)
## workflow encapsula losp pedazos del proceso, receta , modelo etc 
## es una buena practica 

college_cv <- vfold_cv(college_train, v = 5) #5 particiones debido a que son pocos datos. 
lambda_grid <- grid_regular(penalty(c(-5,3)),  levels = 50) #Definimos la grilla

ridge_grid <- tune_grid( #Realizamos el ajuste, pensar que estamos haciendo 5*50=250 ajustes!
  tune_wf,
  resamples = college_cv,
  grid = lambda_grid
)

lowest_rmse <- ridge_grid |>
  select_best(metric = "rmse")


```

bind_rows()
2) de la otra form


```{r} 
lm_mod <-
  parsnip::linear_reg() %>% # Paso 1:  Especificamos el tipo de modelo
  parsnip::set_engine("lm") # Paso 2: Especificamos el motor (engine)
lm_mod

lm_fit = lm_mod %>%  # modelo de parnsip 
  parsnip::fit(Apps ~ ., ## formula
       
                       data = College) # data frame


broom::augment(lm_fit,
               new_data = college_train) 


broom::augment(lm_fit,
               new_data = college_test)




### natalia 

lm_spec = parnsip:linear_reg() %>%  set_engine("lm")

college_lm_fit = lm_spec %>%  parnsip::fit(Apps ~., data = college_train) %>%  bind_rows(training = augment(college_lm_fit, new_data = college_train) %>%  rmse(Apps,.pred), 
                                                                                  ) 



## especificamos usando workflow()
lm_mod = linear_reg() %>%  set_engine("lm")

a =   workflow() |>
  add_formula(Apps ~ .,) |>
  add_model(lm_mod)
  ?add_formula
  
  
fit = a %>%  fit(data = college_train)

tidy(train)

 bind_rows(
   training = augment(fit,new_data = college_train ) %>%  rmse(Apps, .pred), 
   testing   = augment( fit, college_test) %>%  rmse(Apps,.pred))
  

  
```
3) 
## se usa otro metodo 
```{r}

ridge_spec  <-
 linear_reg(penalty = 0, mixture=0)|>   

 set_engine("glmnet") 

ridge_fit = ridge_spec %>%  fit(Apps~., data = college_train)





```






4) Ajustar un modelo lineal usando Lasso, Escoger el lambda con CV. 
Reportar el error obtenido en el conjunto de testeo.


```{r}
tune_lasso <- linear_reg(penalty = tune(), mixture = 1) |> #Con tune() indicamos parámetro a ser ajustado.
  set_engine("glmnet") 

college_lasso_workflow <- workflow() |>
  add_recipe(college_rec) |>
  add_model(tune_lasso)



```


5) Ajustar un modelo de vecinos mas cercanos. Escoger un k con CV . 
Reportrar el error obtenido en el conjunto de testeo 


```{r}
college_recipe = recipes::recipe(Apps~., data = college_train) %>%  recipes::step_dummy(all_nominal_predictors()) %>%  recipes:: step_normalize(all_predictors())

## hay que sacarle las categoricas 

lm_fit = tune_lasso %>%  # modelo de parnsip 
  parsnip::fit(Apps ~ ., ## formula
       
                       data = college_recipe)

```

6) 

poner en la malla 
penalty(c(0.1,100), trans = NULL)














